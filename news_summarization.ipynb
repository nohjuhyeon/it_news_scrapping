{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fa3020-2c13-49a7-9e82-a571e47ca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258910fc-10d8-41f1-b20c-751589f57376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fee2c8-3020-4ecc-9853-47322d578ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3feb7bb-b2f6-468f-9dc7-5e758adec96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/lovit/textrank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94adf5ed-4e78-46f8-8929-67078eb98036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae798671-0a82-4adf-9201-54b8c2cf3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB URI를 입력합니다.\n",
    "uri = \"mongodb+srv://njh2720:shwngus12!@news_scraping.8a8461o.mongodb.net/news_scraping?retryWrites=true&w=majority\"\n",
    "mongo_uri =uri + \"&tlsAllowInvalidCertificates=true\"\n",
    "\n",
    "# MongoDB 클라이언트 생성\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "# 데이터베이스와 컬렉션 선택\n",
    "db = client['news_scraping']\n",
    "collection = db['statistic_bank']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b435043-a6c9-4eb9-b96c-8ca46ca4c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 모든 문서 가져오기\n",
    "documents = collection.find()\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(list(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b0409-47ed-4866-b981-669cd8a64c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b05954-435c-4524-bdc6-217aba26cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "# 기존 캐시를 지우고 새로 다운로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"ainize/kobart-news\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"ainize/kobart-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9142ef6-063b-4e2a-8008-11d6f20c3f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n",
      "1312\n",
      "비트코인과 이더리움 현물 ETF 출시 뒤 다음 타자로 거론되는 솔라나는 1년 새 무려 640%가량 가격이 올라 34.57% 수익률을 기록하고 있다고 24일 가상자산 시황사이트 코인마켓캡에 나타났다. \n",
      "\n",
      "\n",
      " 24일 코인마켓캡에 따르면 솔라나는 오전 7시25분 기준 월초 대비 34.57% 수익률을 기록하고 있고 1년 수익률은 639.86%로 비트코인(125.79%), 이더리움(87.02%)을 앞지른다. \n",
      "1286\n",
      "1300\n",
      "28일 중소벤처기업부가 발표한 ‘국내 딥테크 스타트업 동향’에 따르면 올해 상반기 인공지능(AI) 등 딥테크 분야에 대한 신규 투자는 1조 2000억원 규모로 전년동기대비 80% 증가, 전체 벤처 투자액이 19% 증가한 것과 비교하면 압도적인 성장세로 국내 벤처투자 시장 회복세를 주도하고 있다. \n",
      "1928\n",
      "829 1179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 각 문서에 대해 요약을 생성하고 업데이트\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[0;32m---> 97\u001b[0m     first_summary, full_summary \u001b[38;5;241m=\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(full_summary)\n\u001b[1;32m     99\u001b[0m     update_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$set\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: first_summary,\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m: full_summary\n\u001b[1;32m    103\u001b[0m         }\n\u001b[1;32m    104\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[81], line 66\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     63\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(i, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 요약 생성, 이상한 단어 방지를 위해 repetition_penalty 및 no_repeat_ngram_size 적용\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m summary_text_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 요약문 디코딩\u001b[39;00m\n\u001b[1;32m     75\u001b[0m full_summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_text_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:2883\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2880\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2881\u001b[0m )\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2883\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   2886\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1460\u001b[0m, in \u001b[0;36mBartForConditionalGeneration._reorder_cache\u001b[0;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1456\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1460\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(past_state\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, beam_idx) \u001b[38;5;28;01mfor\u001b[39;00m past_state \u001b[38;5;129;01min\u001b[39;00m layer_past[:\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m   1461\u001b[0m     )\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1460\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1456\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1460\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\u001b[43mpast_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m past_state \u001b[38;5;129;01min\u001b[39;00m layer_past[:\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m   1461\u001b[0m     )\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bson import ObjectId\n",
    "\n",
    "# 텍스트 전처리 및 요약 함수 정의\n",
    "def summary(row):\n",
    "    # 문장을 개행을 기준으로 나누기\n",
    "    # 1차적으로 '\\n' 기준으로 분리\n",
    "    paragraphs = row.split('\\n')\n",
    "\n",
    "    # 2차적으로 각 문단을 '다.' 기준으로 분리\n",
    "    # 2차적으로 각 문단을 '다.' 기준으로 분리한 후, 하나의 리스트로 합침\n",
    "    input_list = []\n",
    "    for paragraph in paragraphs:\n",
    "        if '다.' in paragraph:\n",
    "            sentences = re.findall(r'.*?다\\.', paragraph)\n",
    "            input_list.extend(sentences)  # 리스트를 하나로 합침\n",
    "    \n",
    "    # input_list = row.split('\\n')\n",
    "    # while '' in input_list:\n",
    "    #     input_list.remove('')\n",
    "    # print(input_list)\n",
    "    print(len(row))\n",
    "    partition = int(len(row)//1800) + 1\n",
    "    if partition == 1:\n",
    "        input_list = [' '.join(input_list)]\n",
    "        print(len(input_list[0]))\n",
    "    elif partition == 2:\n",
    "        input_list = [' '.join(input_list[:int(len(input_list)/2)]),' '.join(input_list[int(len(input_list)/2)-2:])]\n",
    "        print(len(input_list[0]),len(input_list[1]))\n",
    "    elif partition == 3:\n",
    "        input_list = [' '.join(input_list[:int(len(input_list)/3)]),' '.join(input_list[int(len(input_list)/3)-2:int(2*len(input_list)/3)]),' '.join(input_list[int(2*len(input_list)/3)-2:])]\n",
    "        print(len(input_list[0]),len(input_list[1]),len(input_list[2]))\n",
    "\n",
    "    elif partition == 4:\n",
    "        input_list = [' '.join(input_list[:int(len(input_list)/4)]),' '.join(input_list[int(len(input_list)/4)-2:int(2*len(input_list)/4)]),' '.join(input_list[int(2*len(input_list)/4)-2:int(3*len(input_list)/4)]),' '.join(input_list[int(3*len(input_list)/4)-2:])]\n",
    "        print(len(input_list[0]),len(input_list[1]),len(input_list[2]),len(input_list[3]))\n",
    "    elif partition == 5:\n",
    "        input_list = [' '.join(input_list[:int(len(input_list)/5)]),' '.join(input_list[int(len(input_list)/5)-2:int(2*len(input_list)/5)]),' '.join(input_list[int(2*len(input_list)/5)-2:int(3*len(input_list)/5)]),' '.join(input_list[int(3*len(input_list)/5)-2:int(4*len(input_list)/5)]),' '.join(input_list[int(4*len(input_list)/5)-2:])]\n",
    "        print(len(input_list[0]),len(input_list[1]),len(input_list[2]),len(input_list[3]),len(input_list[4]))\n",
    "        \n",
    "        \n",
    "    # split_list = [' '.join(input_list[:int(len(input_list)/3)]),' '.join(input_list[int(len(input_list)/3)-2:int(2*len(input_list)/3)]),' '.join(input_list[int(2*len(input_list)/3)-2:])]\n",
    "    # if len(split_list[0]) > 2300 or len(split_list[1]) > 2300 or len(split_list[2]) > 2300:\n",
    "    #     split_list = [' '.join(input_list[:int(len(input_list)/4)]),' '.join(input_list[int(len(input_list)/4)-2:int(2*len(input_list)/4)]),' '.join(input_list[int(2*len(input_list)/4)-2:int(3*len(input_list)/4)]),' '.join(input_list[int(3*len(input_list)/4)-2:])]\n",
    "    #     if len(split_list[0]) > 2300 or len(split_list[1]) > 2300 or len(split_list[2]) > 2300 or len(split_list[3]) > 2300:\n",
    "    #         print(len(' '.join(input_list[:int(len(input_list)/5)])),len(' '.join(input_list[int(len(input_list)/5)-2:int(2*len(input_list)/5)])),len(' '.join(input_list[int(2*len(input_list)/5)-2:int(3*len(input_list)/5)])),len(' '.join(input_list[int(3*len(input_list)/5)-2:int(4*len(input_list)/5)])),len(' '.join(input_list[int(4*len(input_list)/5)-2:])))\n",
    "    #         input_list = [' '.join(input_list[:int(len(input_list)/5)]),' '.join(input_list[int(len(input_list)/5)-2:int(2*len(input_list)/5)]),' '.join(input_list[int(2*len(input_list)/5)-2:int(3*len(input_list)/5)]),' '.join(input_list[int(3*len(input_list)/5)-2:int(4*len(input_list)/5)]),' '.join(input_list[int(4*len(input_list)/5)-2:])]\n",
    "    #     else:\n",
    "    #         print(len(split_list[0]),len(split_list[1]),len(split_list[2]),len(split_list[3]))\n",
    "    #         # print(len(' '.join(input_list[:int(len(input_list)/4)])),len(' '.join(input_list[int(len(input_list)/4)-2:int(2*len(input_list)/4)])),len(' '.join(input_list[int(2*len(input_list)/4)-2:int(3*len(input_list)/4)])),len(' '.join(input_list[int(3*len(input_list)/4):])))\n",
    "    #         input_list = split_list\n",
    "    # else:\n",
    "    #     print(len(split_list[0]),len(split_list[1]),len(split_list[2]))\n",
    "    #     # print(len(' '.join(input_list[:int(len(input_list)/3)])),len(' '.join(input_list[int(len(input_list)/3)-2:int(2*len(input_list)/3)])),len(' '.join(input_list[int(2*len(input_list)/3):])))\n",
    "    #     input_list = split_list\n",
    "        \n",
    "    # 문장을 겹치게 하여 3부분으로 나누기\n",
    "\n",
    "    summary_first = ''\n",
    "    \n",
    "    # 각 부분에 대해 요약 수행\n",
    "    for i in input_list:\n",
    "        input_ids = tokenizer.encode(i, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # 요약 생성, 이상한 단어 방지를 위해 repetition_penalty 및 no_repeat_ngram_size 적용\n",
    "        summary_text_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            length_penalty=1.0,\n",
    "            max_length=250,\n",
    "            min_length=56,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        \n",
    "        # 요약문 디코딩\n",
    "        full_summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "        summary_first += full_summary + ' '\n",
    "    \n",
    "    # 최종 요약\n",
    "    if len(input_list) != 1:\n",
    "        input_ids = tokenizer.encode(summary_first[:-1], truncation=True, return_tensors='pt')\n",
    "        summary_text_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            length_penalty=1.0,\n",
    "            max_length=250,\n",
    "            min_length=56,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        \n",
    "        full_summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        full_summary = summary_first\n",
    "    return summary_first, full_summary \n",
    "\n",
    "# 각 문서에 대해 요약을 생성하고 업데이트\n",
    "\n",
    "for i in range(len(df)):\n",
    "    first_summary, full_summary = summary(df['news_content'][i])\n",
    "    print(full_summary)\n",
    "    update_content = {\n",
    "        \"$set\": {\n",
    "            \"first_summary\": first_summary,\n",
    "            \"full_summary\": full_summary\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 문서 업데이트\n",
    "    result = collection.update_one(\n",
    "        {\"_id\": ObjectId(df['_id'][i])},  # 필터 조건 (문서 ID로 찾기)\n",
    "        update_content  # 업데이트 내용\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba36ce0-9770-4487-a611-d20ee044f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "split_summary = kss.split_sentences(full_summary)\n",
    "\n",
    "# 나눈 문장을 결합하여 출력\n",
    "final_output = ' '.join(split_summary)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b018a-985a-4733-bfd3-0a3d6c912946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length):\n",
    "    \"\"\" 긴 텍스트를 최대 길이로 나누는 함수 \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=True)\n",
    "    return [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "def text_summarization(input_text):\n",
    "    input_text = input_text.replace('\\n', ' ')\n",
    "    chunks = split_text(input_text, max_length=1024)\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_ids = torch.tensor([chunk])\n",
    "        summary_text_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            length_penalty=0.7,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        summaries.append(tokenizer.decode(summary_text_ids[0], skip_special_tokens=True))\n",
    "    \n",
    "    # Concatenate summaries\n",
    "    full_summary = ' '.join(summaries)\n",
    "    # print(full_summary)\n",
    "    return full_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c499124-27a4-46a2-b2d3-753fcecee88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['news_content'].apply(text_summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33faceb-fa0e-4887-be4a-8ad5e1504932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 원본 텍스트 (여기에 데이터프레임에서 가져온 텍스트를 사용하세요)\n",
    "text = df['news_content'][0].split('\\n')\n",
    "\n",
    "# 문장 분리 함수\n",
    "def split_sentences(text):\n",
    "    sentences = []\n",
    "    # 문장 구분 기호 (마침표, 물음표, 느낌표)와 그 뒤의 공백 제거\n",
    "    for i in text:\n",
    "        sentences.extend(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', i))\n",
    "    return sentences\n",
    "\n",
    "# 문장 분리\n",
    "sentences = split_sentences(text)\n",
    "\n",
    "# 결과 출력\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     print(f\"{i+1}. {sentence.strip()}\")\n",
    "' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "600900d4-0ca9-4020-a87a-bbcf062d976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from textrank import KeysentenceSummarizer\n",
    "komoran = Komoran()\n",
    "def komoran_tokenizer(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words\n",
    "\n",
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = komoran_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "keysents = summarizer.summarize(sentences, topk=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6ab6f5a-52f1-4ddb-b7da-5acf15868746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표(오른쪽)와 나권중 이사가 포즈를 취하고 있다./염현아 기자\n",
      "국내 인공지능(AI) 신약개발 업체인 포트래이는 ‘공간 전사체’ 분석 기술을 활용한다.\n",
      "RNA가 어디에 있다면 그곳에서 단백질을 만든다고 볼 수 있다.\n",
      "공간 전사체가 있으면 RNA를 통해 몸 어디에서 단백질이 집중적으로 만들어지는지 알 수 있다.\n",
      "그 단백질이 암과 관련이 있다면 RNA 분포도로 암 세포가 어디에 있는지 알 수 있다.\n",
      "RNA 지도 역할을 하는 공간 전사체를 통해 약물이 어디로 가야 하는지 목적지도 알 수 있다.\n",
      "포트래이를 창업한 사람은 서울대 출신 네 사람이다.\n",
      "이들은 2021년 7월, RNA 지도를 ‘그리고’ ‘전달’한다는 의미의 단어인 ‘Portray’와 ‘AI(인공지능)’를 따 ‘포트래이(Portrai)’라는 스타트업을 공동창업했다.\n",
      "그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다.\n",
      "나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다.\n",
      "포트래이가 보유한 국내외 암 환자의 공간 전사체 데이터 조각을 이어 붙여 하나의 콜라주 작품을 만들었다./포트래이\n",
      "포트래이는 공간 전사체를 활용해 항암 신약을 연구개발(R&D)하는 연구기관이나 제약사들에게 약물의 목적지를 찾아주는 서비스를 제공하고 있다.\n",
      "이 대표는 “우리는 공간 전사체라는 지도에서 암 세포, 즉 항암 치료 약물이 가야 할 목적지를 찾기 위한 데이터 분석 서비스를 제공한다”며 “현재 국내외 고객사는 10곳이 넘는다”고 말했다.\n",
      "공간 전사체를 잘 활용하고 분석 정확도를 높이려면 가장 중요한 건 데이터다.\n",
      "포트래이는 국내와 해외 병원을 통해 암 환자 1000명 이상의 데이터를 보유하고 있으며, 내년에는 그 수가 2000명에 달할 전망이다.\n",
      "포트래이는 공간 전사체를 통해 항암 치료를 위한 후보물질도 자체 발굴했다.\n",
      "지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표와 나권중 이사를 만났다.\n",
      "정말 좋은 신약과 치료법을 개발하면 범용적으로 모든 환자들에게 적용할 수 있으니, 훨씬 더 가치가 있겠다고 늘 생각해 왔다.\n",
      "–포트래이의 주력 사업은 뭔가.\n",
      "물질을 발굴하기 전이든 발굴 후 임상시험 단계든 고객사가 알고 싶어 하는 데이터를 공간 전사체를 활용해 AI 시스템으로 분석해 제공하는 서비스가 사업의 한 축이다.\n",
      "예를 들면, 우리가 개발한 약물이 암 세포에 얼마나 가까이 도달하는지, 또는 특정한 기능을 조절하기 위해 세포 근처에 있는 특정 세포만 죽일 수 있는지 등 의뢰 내용이 다양하다.\n",
      "–포트래이의 강점은 뭔가.\n",
      "이: “공간 전사체가 지도 역할을 한다는 것은 몸 속에 들어간 약물이 우리 혈관이나 암 세포 같은 인체 조직에서 멀리 있는지, 가까이 있는지 등 거리를 알 수 있다는 의미다.\n",
      "기존에는 어디까지나 추정 하에 약물을 투여한 뒤 동물 또는 인체에서 약효 반응이 있는지를 관찰해야 했지만, 이제 데이터 분석으로 미리 알아볼 수 있다.\n",
      "그만큼 암 세포에 정확히 도달하도록 약물을 설계할 수 있다는 의미다.\n",
      "포트래이에 프로젝트를 의뢰한 고객사들마다 거리 개념을 알게 돼 좋다고 하더라.”\n",
      "이: “국내에는 공간 전사체를 연구하는 기관은 있지만, 포트래이처럼 이 기술을 신약에 바로 적용하는 형태의 회사는 아직 없는 것으로 알고 있다.\n",
      "해외에는 2곳 정도 있는데, 모두 시작한 시기는 포트래이와 크게 차이 나지 않는다.\n",
      "다만 프랑스 오킨(Owkin)이라는 회사는 수백억원 규모를 투자해 공간 전사체 데이터를 모아 신약개발에 활용하겠다고 발표한 상태다.”\n",
      "내년에는 포트래이가 직접 끌고 가는 약물이 실제로 생기는 건데, 전임상을 넘어 실제 인체 데이터를 얻는 게 장기적인 목표다.”\n"
     ]
    }
   ],
   "source": [
    "sorted_keysents = sorted(keysents, key=lambda x: x[0])  # 첫 번째 요소가 인덱스이므로 이를 기준으로 정렬\n",
    "\n",
    "# 결과 출력: 원래 문서의 순서대로 요약된 문장 출력\n",
    "for idx, score, sent in sorted_keysents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0b123b0-b7ca-4bfb-8d23-b9302a79b883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표(오른쪽)와 나권중 이사가 포즈를 취하고 있다./염현아 기자 국내 인공지능(AI) 신약개발 업체인 포트래이는 ‘공간 전사체’ 분석 기술을 활용한다. RNA가 어디에 있다면 그곳에서 단백질을 만든다고 볼 수 있다. 공간 전사체가 있으면 RNA를 통해 몸 어디에서 단백질이 집중적으로 만들어지는지 알 수 있다. 그 단백질이 암과 관련이 있다면 RNA 분포도로 암 세포가 어디에 있는지 알 수 있다. RNA 지도 역할을 하는 공간 전사체를 통해 약물이 어디로 가야 하는지 목적지도 알 수 있다. 포트래이를 창업한 사람은 서울대 출신 네 사람이다. 이들은 2021년 7월, RNA 지도를 ‘그리고’ ‘전달’한다는 의미의 단어인 ‘Portray’와 ‘AI(인공지능)’를 따 ‘포트래이(Portrai)’라는 스타트업을 공동창업했다. 그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다. 나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다. 포트래이가 보유한 국내외 암 환자의 공간 전사체 데이터 조각을 이어 붙여 하나의 콜라주 작품을 만들었다./포트래이 포트래이는 공간 전사체를 활용해 항암 신약을 연구개발(R&D)하는 연구기관이나 제약사들에게 약물의 목적지를 찾아주는 서비스를 제공하고 있다. 이 대표는 “우리는 공간 전사체라는 지도에서 암 세포, 즉 항암 치료 약물이 가야 할 목적지를 찾기 위한 데이터 분석 서비스를 제공한다”며 “현재 국내외 고객사는 10곳이 넘는다”고 말했다. 공간 전사체를 잘 활용하고 분석 정확도를 높이려면 가장 중요한 건 데이터다. 포트래이는 국내와 해외 병원을 통해 암 환자 1000명 이상의 데이터를 보유하고 있으며, 내년에는 그 수가 2000명에 달할 전망이다. 포트래이는 공간 전사체를 통해 항암 치료를 위한 후보물질도 자체 발굴했다. 지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표와 나권중 이사를 만났다. 정말 좋은 신약과 치료법을 개발하면 범용적으로 모든 환자들에게 적용할 수 있으니, 훨씬 더 가치가 있겠다고 늘 생각해 왔다. –포트래이의 주력 사업은 뭔가. 물질을 발굴하기 전이든 발굴 후 임상시험 단계든 고객사가 알고 싶어 하는 데이터를 공간 전사체를 활용해 AI 시스템으로 분석해 제공하는 서비스가 사업의 한 축이다. 예를 들면, 우리가 개발한 약물이 암 세포에 얼마나 가까이 도달하는지, 또는 특정한 기능을 조절하기 위해 세포 근처에 있는 특정 세포만 죽일 수 있는지 등 의뢰 내용이 다양하다. –포트래이의 강점은 뭔가. 이: “공간 전사체가 지도 역할을 한다는 것은 몸 속에 들어간 약물이 우리 혈관이나 암 세포 같은 인체 조직에서 멀리 있는지, 가까이 있는지 등 거리를 알 수 있다는 의미다. 기존에는 어디까지나 추정 하에 약물을 투여한 뒤 동물 또는 인체에서 약효 반응이 있는지를 관찰해야 했지만, 이제 데이터 분석으로 미리 알아볼 수 있다. 그만큼 암 세포에 정확히 도달하도록 약물을 설계할 수 있다는 의미다. 포트래이에 프로젝트를 의뢰한 고객사들마다 거리 개념을 알게 돼 좋다고 하더라.” 이: “국내에는 공간 전사체를 연구하는 기관은 있지만, 포트래이처럼 이 기술을 신약에 바로 적용하는 형태의 회사는 아직 없는 것으로 알고 있다. 해외에는 2곳 정도 있는데, 모두 시작한 시기는 포트래이와 크게 차이 나지 않는다. 다만 프랑스 오킨(Owkin)이라는 회사는 수백억원 규모를 투자해 공간 전사체 데이터를 모아 신약개발에 활용하겠다고 발표한 상태다.” 내년에는 포트래이가 직접 끌고 가는 약물이 실제로 생기는 건데, 전임상을 넘어 실제 인체 데이터를 얻는 게 장기적인 목표다.”'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([i[2] for i in sorted_keysents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daa17ea7-e072-484a-a8a9-6b50ecac3bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'최 인공지능(AI) 신약개발 업체인 포트래이는 ‘공간 전사체’ 분석 기술을 활용하여 항암 신약을 연구개발하는 연구기관이나 제약사들에게 약물의 목적지를 찾아주는 데이터 분석 서비스를 제공하고 있으며 내년에는 그 수가 2000명에 달할 전망이다.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(' '.join([i[2] for i in sorted_keysents]), truncation=True, return_tensors='pt')\n",
    "summary_text_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    length_penalty=1.0,\n",
    "    max_length=250,\n",
    "    min_length=56,\n",
    "    num_beams=4,\n",
    "    repetition_penalty=4.0,  # 반복 단어 방지\n",
    "    no_repeat_ngram_size=3,  # 3-gram 반복 방지\n",
    ")\n",
    "\n",
    "full_summary = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71cc5e60-b033-45cb-a6b0-57fb04192580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('전사체/NNP', 4.189310099204426),\n",
       " ('이/NNP', 3.9663774729291585),\n",
       " ('래/NNG', 3.747732715102115),\n",
       " ('포트/NNP', 3.747732715102114),\n",
       " ('공간/NNG', 3.4075894988615487),\n",
       " ('데이터/NNG', 2.898403150821363),\n",
       " ('세포/NNP', 2.8052361091538023),\n",
       " ('약물/NNG', 2.7281382708235635),\n",
       " ('암/NNG', 2.6777513785482716),\n",
       " ('개발/NNG', 2.417079114372705),\n",
       " ('대표/NNG', 2.1135848888592284),\n",
       " ('분석/NNG', 2.104715303762221),\n",
       " ('신약/NNG', 2.0272516926109185),\n",
       " ('교수/NNG', 1.7934272165327103),\n",
       " ('활용/NNG', 1.7883454350705383),\n",
       " ('제공/NNG', 1.5425818844605672),\n",
       " ('기술/NNG', 1.4924197098937495),\n",
       " ('서비스/NNG', 1.4336061270095612),\n",
       " ('항암/NNG', 1.4281131092147086),\n",
       " ('발굴/NNG', 1.297756544895631)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer\n",
    "def komoran_tokenizer(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NNG' in w or '/NNP' in w or '/XR' in w)]\n",
    "    # words = [w for w in words if ('/NN' in w or '/XR' in w)and('/NNB' not in w)]\n",
    "    return words\n",
    "    \n",
    "summarizer = KeywordSummarizer(tokenize=komoran_tokenizer, min_count=2, min_cooccurrence=1)\n",
    "summarizer.summarize(sentences, topk=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "abc84388-e956-4904-aa81-c92c7137eb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표(오른쪽)와 나권중 이사가 포즈를 취하고 있다./염현아 기자 바이러스, 암, 치매 등 모든 질병은 유전자의 발현을 조절하는 과정에서 발생한다. 그 중에서도 특히 DNA 속 유전자 정보가 단백질을 만들기 전에 꼭 거치는 RNA(리보핵산) 생성 단계가 결정적이다. 과학자들은 이 유전자를 어떻게 조절하는지 알면 대부분의 질병을 극복할 수 있다고 보고 있다. 그러기 위해서는 RNA가 우리 몸 속 어디에서 무얼 하는지 알면 된다. 이 때문에 현재 국내외 많은 제약사들이 이 RNA에 주목하고 있다. 신종 코로나바이러스 감염증(코로나19) 백신을 만든 미국 화이자와 모더나도 바로 이 RNA를 이용했다. 국내 인공지능(AI) 신약개발 업체인 포트래이는 ‘공간 전사체’ 분석 기술을 활용한다. 공간 전사체는 RNA의 활동 양상을 세포 단위로 보여주는 지도다. DNA는 인체 모든 생명활동을 관장하는 단백질을 만들 유전정보를 갖고 있다. 단백질이 필요하면 DNA 중 그에 해당하는 정보가 RNA로 복사된다. RNA는 이 정보로 단백질을 만든다. RNA가 어디에 있다면 그곳에서 단백질을 만든다고 볼 수 있다. 포트래이는 암 치료에 주목하고 있다. 공간 전사체가 있으면 RNA를 통해 몸 어디에서 단백질이 집중적으로 만들어지는지 알 수 있다. 그 단백질이 암과 관련이 있다면 RNA 분포도로 암 세포가 어디에 있는지 알 수 있다. RNA 지도 역할을 하는 공간 전사체를 통해 약물이 어디로 가야 하는지 목적지도 알 수 있다. 암이나 약물의 위치를 알려주는 RNA 내비게이션인 셈이어서 암 치료제 개발에 도움이 된다. 포트래이를 창업한 사람은 서울대 출신 네 사람이다. 이들은 2021년 7월, RNA 지도를 ‘그리고’ ‘전달’한다는 의미의 단어인 ‘Portray’와 ‘AI(인공지능)’를 따 ‘포트래이(Portrai)’라는 스타트업을 공동창업했다. 이대승 안과 전문의가 대표(CEO)를 맡고, 최홍윤 서울대병원 핵의학과 교수가 최고기술책임자(CTO)를, 임형준 서울대 융합과학기술대학원 교수는 최고과학책임자(CSO), 나권중 서울대병원 심장혈관흉부외과 교수는 최고의학책임자(CMO)다. 모두 서울대 의대를 나온 의사들이다. 이대승 대표는 블록체인 기술에 관심이 있어 IT(정보기술) 업계에서도 경험을 쌓기도 했다. 그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다. 나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다.',\n",
       " '그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다. 나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다. 포트래이가 보유한 국내외 암 환자의 공간 전사체 데이터 조각을 이어 붙여 하나의 콜라주 작품을 만들었다./포트래이 포트래이는 공간 전사체를 활용해 항암 신약을 연구개발(R&D)하는 연구기관이나 제약사들에게 약물의 목적지를 찾아주는 서비스를 제공하고 있다. 이 대표는 “우리는 공간 전사체라는 지도에서 암 세포, 즉 항암 치료 약물이 가야 할 목적지를 찾기 위한 데이터 분석 서비스를 제공한다”며 “현재 국내외 고객사는 10곳이 넘는다”고 말했다. 공간 전사체를 잘 활용하고 분석 정확도를 높이려면 가장 중요한 건 데이터다. 포트래이는 국내와 해외 병원을 통해 암 환자 1000명 이상의 데이터를 보유하고 있으며, 내년에는 그 수가 2000명에 달할 전망이다. 포트래이는 공간 전사체를 통해 항암 치료를 위한 후보물질도 자체 발굴했다. 모든 고형암에서 효능을 보일 것으로 보고, 자체적인 동물·세포실험을 통해 기술이전을 준비할 예정이다. 지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표와 나권중 이사를 만났다. 다음은 일문일답. –창업을 결심한 계기가 뭔가. 이: “안과 전문의이긴 하지만 사실 의사라는 정체성은 이제 많이 흐릿해졌다. 오히려 IT가 더 강하다. 2015년 국군수도병원에서 안과장으로 군의관 복무를 하던 시절 블록체인을 기반으로 아프리카를 돕는 시스템을 개발하는 NGO(비정부기구) 활동에 관심이 많았다. 당시 우간다 사람들이 글을 써서 소셜미디어에 올리면 암호화폐로 보상을 받는 블록체인 기술을 개발했는데, 2년 동안 총 60명을 지원했더라. 그렇게 지원 받은 사람들이 나중에 은행이나 기업에 취업했다는 연락을 받았을 때 IT 기술이 사람의 인생을 바꿀 수 있구나 깨달았다. 경영을 배우고 싶어서 MBA(경영전문대학원) 학위도 따 보니, 회사는 무조건 돈을 벌어야 되겠더라. 근데 베프(베스트 프렌드)인 나권중 이사가 공간 전사체를 소개해줬을 때 이건 무조건 돈을 벌 수밖에 없겠다고 생각했다.” 나: “평소 하루에 폐암 환자 4명을 수술하고 외래는 40명씩 보고 있는데, 아무리 시간을 쪼개고 에너지를 쏟아부어도 일주일에 가능한 수술은 15명이 최대다. 정말 좋은 신약과 치료법을 개발하면 범용적으로 모든 환자들에게 적용할 수 있으니, 훨씬 더 가치가 있겠다고 늘 생각해 왔다. 그런데 공간 전사체를 알게 되고 나서, 이 기술을 주변에 동료 선배 교수들에게 의견을 물어보니 성장성이 확실히 있었다. 분명 신약개발에 큰 역할을 할 거라고 본다.” –4명이 전공 과목이 다 다른데, 역할 분담이 돼 있나. 나: “전공 과목이 다른 게 굉장한 장점이더라. 기본적으로 프로젝트마다 다같이 소통하고 있는데, 세부적으로 역할은 나눠져 있다. 일단 이 대표는 경영 총괄을 맡아, 고객사와의 소통이나 가장 중요한 투자자 미팅 등을 담당하고, 임형준 이사가 동물·세포실험 관련 업무를 담당한다.',\n",
       " '최홍윤 이사는 생물정보학적인 접근을 맡고, 나는 임상시험을 담당한다. 실험실은 서울대 병원에 구축했다.” –포트래이의 주력 사업은 뭔가. 이: “항암 신약을 개발하는 기관이나 제약사들이 연구개발 과정에서 필요한 데이터들이 많다. 물질을 발굴하기 전이든 발굴 후 임상시험 단계든 고객사가 알고 싶어 하는 데이터를 공간 전사체를 활용해 AI 시스템으로 분석해 제공하는 서비스가 사업의 한 축이다. 예를 들면, 우리가 개발한 약물이 암 세포에 얼마나 가까이 도달하는지, 또는 특정한 기능을 조절하기 위해 세포 근처에 있는 특정 세포만 죽일 수 있는지 등 의뢰 내용이 다양하다. 암종 중에서도 어떤 적응증을 공략해야 할지 고민해서 같은 약물로 5개의 적응증을 분석 비교해 효능 순으로 데이터를 제공한 적도 있다. 현재로선 이 서비스가 매출의 가장 많은 비중을 차지하고 있다. 올해 예상 매출은 15억원 정도다.” –포트래이의 강점은 뭔가. 이: “공간 전사체가 지도 역할을 한다는 것은 몸 속에 들어간 약물이 우리 혈관이나 암 세포 같은 인체 조직에서 멀리 있는지, 가까이 있는지 등 거리를 알 수 있다는 의미다. 기존에는 어디까지나 추정 하에 약물을 투여한 뒤 동물 또는 인체에서 약효 반응이 있는지를 관찰해야 했지만, 이제 데이터 분석으로 미리 알아볼 수 있다. 그만큼 암 세포에 정확히 도달하도록 약물을 설계할 수 있다는 의미다. 포트래이에 프로젝트를 의뢰한 고객사들마다 거리 개념을 알게 돼 좋다고 하더라.” –또 어떤 사업을 하나. 이: “데이터 분석 다음 단계인 치료제 후보물질을 찾는 사업도 실시하고 있다. 최근 자체적으로 후보물질 발굴에 성공해서 현재 전임상 동물실험을 진행하고 있다. 기술이전을 목표로 현재 국내외 기업들과 논의 중이다. 그리고 동물·세포실험을 거쳐 최종 후보물질을 도출해내는 단계까지 도전하려고 한다.” –공간 전사체를 신약개발에 활용하는 업체가 또 있나. 이: “국내에는 공간 전사체를 연구하는 기관은 있지만, 포트래이처럼 이 기술을 신약에 바로 적용하는 형태의 회사는 아직 없는 것으로 알고 있다. 해외에는 2곳 정도 있는데, 모두 시작한 시기는 포트래이와 크게 차이 나지 않는다. 다만 프랑스 오킨(Owkin)이라는 회사는 수백억원 규모를 투자해 공간 전사체 데이터를 모아 신약개발에 활용하겠다고 발표한 상태다.” –올해 목표는 뭔가. 이: “연내 타깃 물질을 찾는 단계에서 함께 개발할 파트너사를 찾는 게 목표다. 내년에는 포트래이가 직접 끌고 가는 약물이 실제로 생기는 건데, 전임상을 넘어 실제 인체 데이터를 얻는 게 장기적인 목표다.”']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = sentences\n",
    "input_list = [' '.join(input_list[:int(len(input_list)/3)]),' '.join(input_list[int(len(input_list)/3)-2:int(2*len(input_list)/3)]),' '.join(input_list[int(2*len(input_list)/3):])]\n",
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0be6529c-7a72-4771-a843-35b8bbc471b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = sentences\n",
    "len(input_list[:int(len(input_list)/3)])\n",
    "len(input_list[int(len(input_list)/3)-2:int(2*len(input_list)/3)])\n",
    "len(input_list[int(2*len(input_list)/3):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9e6d294-ada9-4218-bde1-d039c5491256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표(오른쪽)와 나권중 이사가 포즈를 취하고 있다./염현아 기자 바이러스, 암, 치매 등 모든 질병은 유전자의 발현을 조절하는 과정에서 발생한다. 그 중에서도 특히 DNA 속 유전자 정보가 단백질을 만들기 전에 꼭 거치는 RNA(리보핵산) 생성 단계가 결정적이다. 과학자들은 이 유전자를 어떻게 조절하는지 알면 대부분의 질병을 극복할 수 있다고 보고 있다. 그러기 위해서는 RNA가 우리 몸 속 어디에서 무얼 하는지 알면 된다. 이 때문에 현재 국내외 많은 제약사들이 이 RNA에 주목하고 있다. 신종 코로나바이러스 감염증(코로나19) 백신을 만든 미국 화이자와 모더나도 바로 이 RNA를 이용했다. 국내 인공지능(AI) 신약개발 업체인 포트래이는 ‘공간 전사체’ 분석 기술을 활용한다. 공간 전사체는 RNA의 활동 양상을 세포 단위로 보여주는 지도다. DNA는 인체 모든 생명활동을 관장하는 단백질을 만들 유전정보를 갖고 있다. 단백질이 필요하면 DNA 중 그에 해당하는 정보가 RNA로 복사된다. RNA는 이 정보로 단백질을 만든다. RNA가 어디에 있다면 그곳에서 단백질을 만든다고 볼 수 있다. 포트래이는 암 치료에 주목하고 있다. 공간 전사체가 있으면 RNA를 통해 몸 어디에서 단백질이 집중적으로 만들어지는지 알 수 있다. 그 단백질이 암과 관련이 있다면 RNA 분포도로 암 세포가 어디에 있는지 알 수 있다. RNA 지도 역할을 하는 공간 전사체를 통해 약물이 어디로 가야 하는지 목적지도 알 수 있다. 암이나 약물의 위치를 알려주는 RNA 내비게이션인 셈이어서 암 치료제 개발에 도움이 된다. 포트래이를 창업한 사람은 서울대 출신 네 사람이다. 이들은 2021년 7월, RNA 지도를 ‘그리고’ ‘전달’한다는 의미의 단어인 ‘Portray’와 ‘AI(인공지능)’를 따 ‘포트래이(Portrai)’라는 스타트업을 공동창업했다. 이대승 안과 전문의가 대표(CEO)를 맡고, 최홍윤 서울대병원 핵의학과 교수가 최고기술책임자(CTO)를, 임형준 서울대 융합과학기술대학원 교수는 최고과학책임자(CSO), 나권중 서울대병원 심장혈관흉부외과 교수는 최고의학책임자(CMO)다. 모두 서울대 의대를 나온 의사들이다. 이대승 대표는 블록체인 기술에 관심이 있어 IT(정보기술) 업계에서도 경험을 쌓기도 했다. 그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다. 나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다.\n",
      "Chunk 2: 그러던 중 최홍윤, 나권중 교수가 이 대표를 찾아와 공간 전사체 기술을 활용해 신약개발 연구를 해보자고 제안했다. 나노신약 개발을 하던 임형준 교수도 합류해 약물 분포와 공간 전사체를 같이 분석하자는 아이디어를 냈다. 포트래이가 보유한 국내외 암 환자의 공간 전사체 데이터 조각을 이어 붙여 하나의 콜라주 작품을 만들었다./포트래이 포트래이는 공간 전사체를 활용해 항암 신약을 연구개발(R&D)하는 연구기관이나 제약사들에게 약물의 목적지를 찾아주는 서비스를 제공하고 있다. 이 대표는 “우리는 공간 전사체라는 지도에서 암 세포, 즉 항암 치료 약물이 가야 할 목적지를 찾기 위한 데이터 분석 서비스를 제공한다”며 “현재 국내외 고객사는 10곳이 넘는다”고 말했다. 공간 전사체를 잘 활용하고 분석 정확도를 높이려면 가장 중요한 건 데이터다. 포트래이는 국내와 해외 병원을 통해 암 환자 1000명 이상의 데이터를 보유하고 있으며, 내년에는 그 수가 2000명에 달할 전망이다. 포트래이는 공간 전사체를 통해 항암 치료를 위한 후보물질도 자체 발굴했다. 모든 고형암에서 효능을 보일 것으로 보고, 자체적인 동물·세포실험을 통해 기술이전을 준비할 예정이다. 지난 23일 서울 종로구 포트래이 사옥에서 이대승 대표와 나권중 이사를 만났다. 다음은 일문일답. –창업을 결심한 계기가 뭔가. 이: “안과 전문의이긴 하지만 사실 의사라는 정체성은 이제 많이 흐릿해졌다. 오히려 IT가 더 강하다. 2015년 국군수도병원에서 안과장으로 군의관 복무를 하던 시절 블록체인을 기반으로 아프리카를 돕는 시스템을 개발하는 NGO(비정부기구) 활동에 관심이 많았다. 당시 우간다 사람들이 글을 써서 소셜미디어에 올리면 암호화폐로 보상을 받는 블록체인 기술을 개발했는데, 2년 동안 총 60명을 지원했더라. 그렇게 지원 받은 사람들이 나중에 은행이나 기업에 취업했다는 연락을 받았을 때 IT 기술이 사람의 인생을 바꿀 수 있구나 깨달았다. 경영을 배우고 싶어서 MBA(경영전문대학원) 학위도 따 보니, 회사는 무조건 돈을 벌어야 되겠더라. 근데 베프(베스트 프렌드)인 나권중 이사가 공간 전사체를 소개해줬을 때 이건 무조건 돈을 벌 수밖에 없겠다고 생각했다.” 나: “평소 하루에 폐암 환자 4명을 수술하고 외래는 40명씩 보고 있는데, 아무리 시간을 쪼개고 에너지를 쏟아부어도 일주일에 가능한 수술은 15명이 최대다. 정말 좋은 신약과 치료법을 개발하면 범용적으로 모든 환자들에게 적용할 수 있으니, 훨씬 더 가치가 있겠다고 늘 생각해 왔다. 그런데 공간 전사체를 알게 되고 나서, 이 기술을 주변에 동료 선배 교수들에게 의견을 물어보니 성장성이 확실히 있었다. 분명 신약개발에 큰 역할을 할 거라고 본다.” –4명이 전공 과목이 다 다른데, 역할 분담이 돼 있나. 나: “전공 과목이 다른 게 굉장한 장점이더라.\n",
      "Chunk 3: –4명이 전공 과목이 다 다른데, 역할 분담이 돼 있나. 나: “전공 과목이 다른 게 굉장한 장점이더라. 기본적으로 프로젝트마다 다같이 소통하고 있는데, 세부적으로 역할은 나눠져 있다. 일단 이 대표는 경영 총괄을 맡아, 고객사와의 소통이나 가장 중요한 투자자 미팅 등을 담당하고, 임형준 이사가 동물·세포실험 관련 업무를 담당한다. 최홍윤 이사는 생물정보학적인 접근을 맡고, 나는 임상시험을 담당한다. 실험실은 서울대 병원에 구축했다.” –포트래이의 주력 사업은 뭔가. 이: “항암 신약을 개발하는 기관이나 제약사들이 연구개발 과정에서 필요한 데이터들이 많다. 물질을 발굴하기 전이든 발굴 후 임상시험 단계든 고객사가 알고 싶어 하는 데이터를 공간 전사체를 활용해 AI 시스템으로 분석해 제공하는 서비스가 사업의 한 축이다. 예를 들면, 우리가 개발한 약물이 암 세포에 얼마나 가까이 도달하는지, 또는 특정한 기능을 조절하기 위해 세포 근처에 있는 특정 세포만 죽일 수 있는지 등 의뢰 내용이 다양하다. 암종 중에서도 어떤 적응증을 공략해야 할지 고민해서 같은 약물로 5개의 적응증을 분석 비교해 효능 순으로 데이터를 제공한 적도 있다. 현재로선 이 서비스가 매출의 가장 많은 비중을 차지하고 있다. 올해 예상 매출은 15억원 정도다.” –포트래이의 강점은 뭔가. 이: “공간 전사체가 지도 역할을 한다는 것은 몸 속에 들어간 약물이 우리 혈관이나 암 세포 같은 인체 조직에서 멀리 있는지, 가까이 있는지 등 거리를 알 수 있다는 의미다. 기존에는 어디까지나 추정 하에 약물을 투여한 뒤 동물 또는 인체에서 약효 반응이 있는지를 관찰해야 했지만, 이제 데이터 분석으로 미리 알아볼 수 있다. 그만큼 암 세포에 정확히 도달하도록 약물을 설계할 수 있다는 의미다. 포트래이에 프로젝트를 의뢰한 고객사들마다 거리 개념을 알게 돼 좋다고 하더라.” –또 어떤 사업을 하나. 이: “데이터 분석 다음 단계인 치료제 후보물질을 찾는 사업도 실시하고 있다. 최근 자체적으로 후보물질 발굴에 성공해서 현재 전임상 동물실험을 진행하고 있다. 기술이전을 목표로 현재 국내외 기업들과 논의 중이다. 그리고 동물·세포실험을 거쳐 최종 후보물질을 도출해내는 단계까지 도전하려고 한다.” –공간 전사체를 신약개발에 활용하는 업체가 또 있나. 이: “국내에는 공간 전사체를 연구하는 기관은 있지만, 포트래이처럼 이 기술을 신약에 바로 적용하는 형태의 회사는 아직 없는 것으로 알고 있다.\n",
      "Chunk 4: –공간 전사체를 신약개발에 활용하는 업체가 또 있나. 이: “국내에는 공간 전사체를 연구하는 기관은 있지만, 포트래이처럼 이 기술을 신약에 바로 적용하는 형태의 회사는 아직 없는 것으로 알고 있다. 해외에는 2곳 정도 있는데, 모두 시작한 시기는 포트래이와 크게 차이 나지 않는다. 다만 프랑스 오킨(Owkin)이라는 회사는 수백억원 규모를 투자해 공간 전사체 데이터를 모아 신약개발에 활용하겠다고 발표한 상태다.” –올해 목표는 뭔가. 이: “연내 타깃 물질을 찾는 단계에서 함께 개발할 파트너사를 찾는 게 목표다. 내년에는 포트래이가 직접 끌고 가는 약물이 실제로 생기는 건데, 전임상을 넘어 실제 인체 데이터를 얻는 게 장기적인 목표다.”\n"
     ]
    }
   ],
   "source": [
    "def sliding_chunk_list(lst, chunk_size, overlap):\n",
    "    \"\"\"슬라이딩 윈도우 방식으로 리스트를 나누는 함수\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(lst):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(lst[start:end])\n",
    "        start = end - overlap  # Overlap을 고려하여 시작점을 조정\n",
    "    return chunks\n",
    "\n",
    "def join_chunks(sentences, chunk_size, overlap):\n",
    "    \"\"\"문장을 chunk_size 크기로 나누고, 각 chunk를 문자열로 변환하는 함수\"\"\"\n",
    "    chunks = sliding_chunk_list(sentences, chunk_size, overlap)\n",
    "    joined_chunks = [' '.join(chunk) for chunk in chunks]\n",
    "    return joined_chunks\n",
    "\n",
    "# 25개씩 나누고 앞뒤로 2개 문장이 겹치게 조정\n",
    "chunk_size = 25\n",
    "overlap = 2\n",
    "result = join_chunks(sentences, chunk_size, overlap)\n",
    "\n",
    "# 결과 확인\n",
    "for i, chunk in enumerate(result):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e13003db-734b-4687-95b8-e013ff1c81a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6d01d-4348-4e0a-b81e-458711a651ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "    sentences = text.split('\\n')\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(sentences), chunk_size - overlap):\n",
    "        chunk = sentences[i:i + chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        if len(chunk) < chunk_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d7f1355-6959-4770-8d80-517a43b7c7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>news_title</th>\n",
       "      <th>news_content</th>\n",
       "      <th>news_date</th>\n",
       "      <th>news_link</th>\n",
       "      <th>first_summary</th>\n",
       "      <th>full_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66d573622b014e9b5dead6c1</td>\n",
       "      <td>이더리움 대항마 ‘솔라나’ 1년새 640% 급등</td>\n",
       "      <td>가상자산 현물ETF 승인 기대 속\\n\\n‘블록거래 수수료 사용’ 매력적\\n\\n비트코...</td>\n",
       "      <td>2024-07-24 01:58:15</td>\n",
       "      <td>https://news.heraldcorp.com/view.php?ud=202407...</td>\n",
       "      <td>솔라나는 24일 오전 7시25분 기준 월초 대비 34.57% 수익률을 기록하고 1년...</td>\n",
       "      <td>미국 자산운용사 반에크는 지난달 27일(현지시간) 미국 증권거래위원회(SEC)에 솔...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66d573662b014e9b5dead6c3</td>\n",
       "      <td>AI 분야 투자 447% 급증…‘딥테크 스타트업’에 돈 몰린다</td>\n",
       "      <td>[이데일리 김경은 기자] 인공지능(AI) 등 딥테크 스타트업에 투자가 몰리며 국내 ...</td>\n",
       "      <td>2024-07-28 03:00:00</td>\n",
       "      <td>https://www.edaily.co.kr/News/Read?newsId=0138...</td>\n",
       "      <td>\\n지난해 한때 한때 취업이 이루어졌음에도 불구하고 취업난에 시달 시달 시달 시달 ...</td>\n",
       "      <td>중중벤처기업부가 발표한 국내 딥테크 스타트업 동향에 따르면 올해 상반기 인공지능(A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66d573a62b014e9b5dead6dd</td>\n",
       "      <td>‘엔고의 저주’ 日증시 12.4% 대폭락… 원엔 환율 964원</td>\n",
       "      <td>[증시 사상최대 폭락]\\n\\n日 금리 인상에 슈퍼엔저 끝나… 엔화 빌려 투자하던 헤...</td>\n",
       "      <td>2024-08-06 12:00:00</td>\n",
       "      <td>https://www.donga.com/news/Inter/article/all/2...</td>\n",
       "      <td>증 증 증 증 증 증 증  증 증 증 증 증 증 증 증 증 증시의의의의의의  지난 ...</td>\n",
       "      <td>주식 증시가 역대 최대 낙폭을 보이며 향후 엔고 현상이 가속화되면 일본의 수출 기업...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66d573732b014e9b5dead6c9</td>\n",
       "      <td>한국, 주 49시간 일하면 빈곤탈출…OECD 평균보다 적어</td>\n",
       "      <td>최저임금 수준 비교\\n\\n한국에서 두 자녀가 있는 최저임금 근로자가 빈곤에서 탈출하...</td>\n",
       "      <td>2024-08-05 15:02:01</td>\n",
       "      <td>https://www.joongang.co.kr/article/25268688</td>\n",
       "      <td>최저임금이 10년 전 주당 80시간보다 크게 줄어든 것으로 최저임금이 가파르게 상승...</td>\n",
       "      <td>최저임금이 10년 전 주당 80시간보다 크게 줄어든 것으로 최저임금이 가파르게 상승...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66d573942b014e9b5dead6d9</td>\n",
       "      <td>동해서 물놀이하다 \"으악!\"… 해파리 주의보</td>\n",
       "      <td>최근 강원 동해안 해수욕장에서 해파리 쏘임 사고가 잇따르고 있는 가운데 22일 강릉...</td>\n",
       "      <td>2024-07-23 00:00:00</td>\n",
       "      <td>https://www.chosun.com/national/regional/2024/...</td>\n",
       "      <td>지난해 강원도에서 발생한 해파리 쏘임 사고는 총 45건이었는데 불과 3일 만에 그 ...</td>\n",
       "      <td>22원도에 따르면, 지난 19일부터 21일까지 3일간 발생한 해파리 쏘임 사고는 총...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>66d58a7b2b014e9b5deadd37</td>\n",
       "      <td>中, 인수자·송금까지 '태클'…\"200억 자산 팔아 손에 쥔건 30억\"</td>\n",
       "      <td>사업철수 발목 잡는 中\\n\\n\\n\\n현지기업과 매각 협상 마쳤는데\\n\\n中 정부 \"...</td>\n",
       "      <td>2023-01-08 09:27:15</td>\n",
       "      <td>https://www.hankyung.com/economy/article/20230...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>66d58a892b014e9b5deadd3f</td>\n",
       "      <td>“우주서 채소 수경재배-인공육 배양”… 푸드테크, 꿈을 현실로</td>\n",
       "      <td>[민간 우주 호모 스페이스쿠스 시대]〈3〉우주 먹거리 개발 활기\\n\\n동물에서 추출...</td>\n",
       "      <td>2023-01-06 11:00:00</td>\n",
       "      <td>https://www.donga.com/news/article/all/2023010...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>66d58a972b014e9b5deadd41</td>\n",
       "      <td>對中 수출쇼크 빈자리 베트남이 채워…韓 무역 버팀목 역할</td>\n",
       "      <td>코로나·공급망 위기가 바꾼 한국 수출지형\\n\\n中 대체할 생산기지 부상\\n\\nOLE...</td>\n",
       "      <td>2023-01-04 08:38:38</td>\n",
       "      <td>https://www.mk.co.kr/news/economy/10593584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>66d58afb2b014e9b5deadd65</td>\n",
       "      <td>10대 파파고, 20대 토스, 30~60대는…</td>\n",
       "      <td>페이스북의 퇴장, 20대를 못 뚫은 삼성페이, 중년 포털이 된 다음, 중년층 공략이...</td>\n",
       "      <td>2022-12-30 00:00:00</td>\n",
       "      <td>https://www.chosun.com/economy/tech_it/2022/12...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>66d58b052b014e9b5deadd69</td>\n",
       "      <td>“테슬라 전기車·가상화폐·메타 VR 헤드셋…내년에 절대 사지 말아라”</td>\n",
       "      <td>서울 롯데백화점 영등포점에 전시된 테슬라 ‘모델Y’의 모습. 2021.1.13/뉴스...</td>\n",
       "      <td>2022-12-29 11:00:00</td>\n",
       "      <td>https://www.donga.com/news/article/all/2022122...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1462 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id                               news_title  \\\n",
       "0     66d573622b014e9b5dead6c1               이더리움 대항마 ‘솔라나’ 1년새 640% 급등   \n",
       "1     66d573662b014e9b5dead6c3       AI 분야 투자 447% 급증…‘딥테크 스타트업’에 돈 몰린다   \n",
       "2     66d573a62b014e9b5dead6dd       ‘엔고의 저주’ 日증시 12.4% 대폭락… 원엔 환율 964원   \n",
       "3     66d573732b014e9b5dead6c9         한국, 주 49시간 일하면 빈곤탈출…OECD 평균보다 적어   \n",
       "4     66d573942b014e9b5dead6d9                 동해서 물놀이하다 \"으악!\"… 해파리 주의보   \n",
       "...                        ...                                      ...   \n",
       "1457  66d58a7b2b014e9b5deadd37  中, 인수자·송금까지 '태클'…\"200억 자산 팔아 손에 쥔건 30억\"   \n",
       "1458  66d58a892b014e9b5deadd3f       “우주서 채소 수경재배-인공육 배양”… 푸드테크, 꿈을 현실로   \n",
       "1459  66d58a972b014e9b5deadd41          對中 수출쇼크 빈자리 베트남이 채워…韓 무역 버팀목 역할   \n",
       "1460  66d58afb2b014e9b5deadd65                10대 파파고, 20대 토스, 30~60대는…   \n",
       "1461  66d58b052b014e9b5deadd69   “테슬라 전기車·가상화폐·메타 VR 헤드셋…내년에 절대 사지 말아라”   \n",
       "\n",
       "                                           news_content           news_date  \\\n",
       "0     가상자산 현물ETF 승인 기대 속\\n\\n‘블록거래 수수료 사용’ 매력적\\n\\n비트코... 2024-07-24 01:58:15   \n",
       "1     [이데일리 김경은 기자] 인공지능(AI) 등 딥테크 스타트업에 투자가 몰리며 국내 ... 2024-07-28 03:00:00   \n",
       "2     [증시 사상최대 폭락]\\n\\n日 금리 인상에 슈퍼엔저 끝나… 엔화 빌려 투자하던 헤... 2024-08-06 12:00:00   \n",
       "3     최저임금 수준 비교\\n\\n한국에서 두 자녀가 있는 최저임금 근로자가 빈곤에서 탈출하... 2024-08-05 15:02:01   \n",
       "4     최근 강원 동해안 해수욕장에서 해파리 쏘임 사고가 잇따르고 있는 가운데 22일 강릉... 2024-07-23 00:00:00   \n",
       "...                                                 ...                 ...   \n",
       "1457  사업철수 발목 잡는 中\\n\\n\\n\\n현지기업과 매각 협상 마쳤는데\\n\\n中 정부 \"... 2023-01-08 09:27:15   \n",
       "1458  [민간 우주 호모 스페이스쿠스 시대]〈3〉우주 먹거리 개발 활기\\n\\n동물에서 추출... 2023-01-06 11:00:00   \n",
       "1459  코로나·공급망 위기가 바꾼 한국 수출지형\\n\\n中 대체할 생산기지 부상\\n\\nOLE... 2023-01-04 08:38:38   \n",
       "1460  페이스북의 퇴장, 20대를 못 뚫은 삼성페이, 중년 포털이 된 다음, 중년층 공략이... 2022-12-30 00:00:00   \n",
       "1461  서울 롯데백화점 영등포점에 전시된 테슬라 ‘모델Y’의 모습. 2021.1.13/뉴스... 2022-12-29 11:00:00   \n",
       "\n",
       "                                              news_link  \\\n",
       "0     https://news.heraldcorp.com/view.php?ud=202407...   \n",
       "1     https://www.edaily.co.kr/News/Read?newsId=0138...   \n",
       "2     https://www.donga.com/news/Inter/article/all/2...   \n",
       "3           https://www.joongang.co.kr/article/25268688   \n",
       "4     https://www.chosun.com/national/regional/2024/...   \n",
       "...                                                 ...   \n",
       "1457  https://www.hankyung.com/economy/article/20230...   \n",
       "1458  https://www.donga.com/news/article/all/2023010...   \n",
       "1459         https://www.mk.co.kr/news/economy/10593584   \n",
       "1460  https://www.chosun.com/economy/tech_it/2022/12...   \n",
       "1461  https://www.donga.com/news/article/all/2022122...   \n",
       "\n",
       "                                          first_summary  \\\n",
       "0     솔라나는 24일 오전 7시25분 기준 월초 대비 34.57% 수익률을 기록하고 1년...   \n",
       "1     \\n지난해 한때 한때 취업이 이루어졌음에도 불구하고 취업난에 시달 시달 시달 시달 ...   \n",
       "2     증 증 증 증 증 증 증  증 증 증 증 증 증 증 증 증 증시의의의의의의  지난 ...   \n",
       "3     최저임금이 10년 전 주당 80시간보다 크게 줄어든 것으로 최저임금이 가파르게 상승...   \n",
       "4     지난해 강원도에서 발생한 해파리 쏘임 사고는 총 45건이었는데 불과 3일 만에 그 ...   \n",
       "...                                                 ...   \n",
       "1457                                                NaN   \n",
       "1458                                                NaN   \n",
       "1459                                                NaN   \n",
       "1460                                                NaN   \n",
       "1461                                                NaN   \n",
       "\n",
       "                                           full_summary  \n",
       "0     미국 자산운용사 반에크는 지난달 27일(현지시간) 미국 증권거래위원회(SEC)에 솔...  \n",
       "1     중중벤처기업부가 발표한 국내 딥테크 스타트업 동향에 따르면 올해 상반기 인공지능(A...  \n",
       "2     주식 증시가 역대 최대 낙폭을 보이며 향후 엔고 현상이 가속화되면 일본의 수출 기업...  \n",
       "3     최저임금이 10년 전 주당 80시간보다 크게 줄어든 것으로 최저임금이 가파르게 상승...  \n",
       "4     22원도에 따르면, 지난 19일부터 21일까지 3일간 발생한 해파리 쏘임 사고는 총...  \n",
       "...                                                 ...  \n",
       "1457                                                NaN  \n",
       "1458                                                NaN  \n",
       "1459                                                NaN  \n",
       "1460                                                NaN  \n",
       "1461                                                NaN  \n",
       "\n",
       "[1462 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4edcafb2-9837-4e13-9dc4-9ecbfabb3861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일본 도쿄의 한 전광판에 역대 최대 하락 폭을 다시 쓴 일본 닛케이평균주가가 표시돼 있다.',\n",
       " '엔 캐리 트레이드(Yen Carry Trade) 낮은 금리로 엔화를 빌려 세계 각지에 투자하는 금융거래. 요즘처럼 일본 금리가 오르면 저렴한 엔화로 사들인 자산을 되팔아야 하기 때문에 금융시장에 충격을 준다.',\n",
       " '일본 증시가 하루 만에 4,400엔 넘게 급락하며 1987년 ‘블랙 먼데이’를 뛰어넘는 사상 최대 낙폭을 보였다.',\n",
       " ' 미국발 ‘R(Recession·경기 침체)의 공포’와 일본 증시 상승을 견인해온 ‘슈퍼 엔저’의 종말이 맞물리면서 일본 대표 지수는 올해 상승분을 모두 토해냈다.',\n",
       " ' 일본은행(BOJ)의 추가 금리 인상 가능성이 제기되는 가운데 향후 엔고(円高) 현상이 가속화되면 일본의 수출 기업들이 큰 타격을 입을 거란 전망도 나온다.',\n",
       " '5일 도쿄 주식시장에서 닛케이평균주가(닛케이지수)는 전 거래일보다 12.4%(4,451엔) 폭락한 31,458.42엔에 마감했다.',\n",
       " ' 3,836엔 떨어졌던 1987년 10월 20일 ‘블랙먼데이’를 뛰어넘는 하락 폭이다.',\n",
       " ' 이날 일본 중견기업 1900개를 포함한 토픽스는 전장보다 12.23%(310.45포인트) 하락한 2,227.15에 거래를 마쳤다.',\n",
       " ' 이날 토픽스와 닛케이지수 선물 거래를 일시 중단하는 ‘서킷브레이커’가 발동됐지만 폭락세를 멈추진 못했다.',\n",
       " '니혼게이자이신문 등은 앞서 2일 발표된 미국의 7월 고용지표 등에 따른 경기 침체 우려가 투자 심리에 악영향을 미쳤다고 분석했다.',\n",
       " ' 신문은 이날 주가 움직임에 대해 “만석인 극장에서 누군가 ‘불이야’를 외쳤을 때와 같은 광경”이었다며 “시장 참가자 전원이 주식 매도로 움직였다”고 전했다.',\n",
       " '여기에 일본이 지난달 2010년부터 이어온 ‘제로(0) 금리’ 정책에서 완전히 벗어나면서 엔화 가치가 급등한 것도 주식시장 불안을 부추겼다.',\n",
       " ' 앞서 일본 중앙은행은 지난달 말 열린 금융정책결정회의에서 정책금리를 현행 0∼0.1%에서 0.25%로 인상했다.',\n",
       " ' 3월 마이너스 금리를 해제한 지 4개월 만이다.',\n",
       " '일본 중앙은행이 금리 인상을 결정한 뒤 그간 지속돼 왔던 ‘슈퍼 엔저’ 시대가 저물고 엔화 가치는 급등하고 있다.',\n",
       " ' 이날 도쿄 외환시장에서 엔-달러 환율은 달러당 141엔대까지 떨어졌다(엔화 가치 상승). 엔-달러 환율은 지난달 초 161.90엔까지 올랐지만 한 달여 만에 20엔 가까이 하락한 것이다.',\n",
       " ' 원-엔 재정 환율도 하루 만에 40원 이상 올라 5일에 100엔당 960원대까지 상승했다.',\n",
       " '‘엔 캐리 트레이드’(싼 이자로 엔화를 빌려 상대적으로 금리가 높은 국가에 투자하는 방법)가 청산 수순을 밟으면서 글로벌 유동성이 축소돼 증시 하락이 더 가팔라졌다는 분석도 나온다.',\n",
       " ' 최보원 한국투자증권 수석연구원은 “일본은 10년 넘게 유지해온 통화 정책을 비로소 바꾼 만큼 정책 변경에 따른 파장을 소화할 시간이 필요하다”며 “슈퍼 엔저가 막을 내리면서 엔 캐리 트레이드 청산 가능성이 높아졌고, 이는 일본 내에서도 위험 자산에 대한 투자 심리를 위축시키는 요인으로 작용한다”고 짚었다.',\n",
       " '헤지펀드 등 글로벌 투자자들은 일본에서 저렴하게 돈을 빌려 미국의 채권이나 주식 등 글로벌 시장에 투자해 왔다.',\n",
       " ' 하지만 일본의 금리 상승으로 이자 비용이 불어나고 엔화 가치 급등으로 인한 손실도 커지면서, 투자자들이 서둘러 자산을 매각하고 빚 갚기에 나서 변동성이 확대됐다는 얘기다.',\n",
       " ' 이경민 대신증권 연구원은 “엔 캐리 트레이드 청산이 글로벌 증시 폭락을 더 부추기고 있다”며 “미국의 경기 침체는 달러화 약세, 엔화 강세로 나타나기 때문에 엔 캐리 트레이드 청산을 더 유도할 것”이라고 말했다.',\n",
       " ' 다만 일각에서는 엔 캐리 트레이드 청산이 본격화하지 않았다면서 글로벌 증시에 미치는 영향력은 제한적이라고 평가하기도 했다.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "answer = re.findall(r'.*?다\\.', df.loc[2, 'news_content'])\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "345429ed-4020-4889-ac3b-aa0996b1a50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['네이트 뉴스 댓글은 기사에 대한 자신의 생각을 말하고 남의 생각을 들으며 서로 다양한 의견을 나누는 공간입니다.',\n",
       " ' 건전한 인터넷 문화 정착을 위해 아래와 같은 댓글 운영원칙을 적용합니다.',\n",
       " '아래에 해당하는 내용이 포함된 댓글은 이용자 신고 등에 의해 별도의 예고 없이 삭제·노출 제한될 수 있습니다.',\n",
       " '불건전한 댓글이 발견되는 경우 별도의 통보 없이 즉시 삭제하며, 상습적이거나 위반 정도가 심한 댓글을 게시하는 분에 대해서는 일정 기간 댓글 작성에 대한 권한을 제한할 수 있습니다.',\n",
       " '또한 비정상적인 패턴으로 댓글 작성, 추천, 반대 등이 발생하는 것으로 관리자 또는 시스템이 판단하는 경우 부정 클릭여부를 확인하여 해당 IP나 ID를 제재할 수 있습니다.',\n",
       " '댓글에 비속어 및 상대방의 불쾌감을 주는 단어를 사용하거나, 유명인이나 특정 일반인 또는 단체나 기관을 사칭하는 경우 제재될 수 있습니다.',\n",
       " '명예훼손, 개인정보 유출, 욕설 등 법률에 위반되는 댓글은 관계 법령에 의거 민형사상의 불이익을 받을 수 있으니 이용에 주의를 부탁드립니다.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = df.loc[34, 'news_content'].split('\\n')\n",
    "\n",
    "# 2차적으로 각 문단을 '다.' 기준으로 분리\n",
    "# 2차적으로 각 문단을 '다.' 기준으로 분리한 후, 하나의 리스트로 합침\n",
    "input_list = []\n",
    "for paragraph in paragraphs:\n",
    "    if '다.' in paragraph:\n",
    "        sentences = re.findall(r'.*?다\\.', paragraph)\n",
    "        input_list.extend(sentences)  # 리스트를 하나로 합침\n",
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca588189-b734-4c3d-a998-20b8e35cfc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
